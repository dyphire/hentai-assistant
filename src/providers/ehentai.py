import re, os, json, time
import requests
from bs4 import BeautifulSoup
from datetime import datetime

from utils import check_dirs

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
}

def get_original_tag(text):
    dictpath = check_dirs(os.path.join('data', 'ehentai', 'translations'))
    tags_json_path = os.path.join(dictpath, 'tags.json')
    if os.path.isfile(tags_json_path):
        with open(tags_json_path, 'r', encoding='utf-8') as rf:
            tagsdict = json.load(rf)
    else:
        tagsdict = {}
    if text in tagsdict:
        return tagsdict[text]
    else:
        print('æœªæ‰¾åˆ°è¯æ¡,æœç´¢åœ¨çº¿å†…å®¹')
        url = 'https://ehwiki.org/wiki/' + text.replace(' ','_')
        response = requests.get(url, headers=headers)
        searchJapanese = re.search(r'Japanese</b>:\s*(.+?)<', response.text)
        if not searchJapanese == None:
            tagsdict[text] = re.sub(' ','',searchJapanese.group(1))
            with open(tags_json_path, 'w', encoding='utf-8') as wf:
                json.dump(tagsdict, wf, ensure_ascii=False, indent=4)
            return tagsdict[text]

def male_only_taglist():
    data_dir = check_dirs(os.path.join("data", "ehentai"))
    json_path = os.path.join(data_dir, "tags", "male_only_taglist.json")
    if os.path.exists(json_path):
        with open(json_path) as f:
            return json.load(f)['content']
    m_list = []
    
    fetish_html_path = os.path.join(data_dir, "fetish_listing.html")
    if not os.path.exists(fetish_html_path):
        url = "https://ehwiki.org/wiki/Fetish_Listing"
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            with open(fetish_html_path, 'w') as f:
                f.write(response.text)
    
    with open(fetish_html_path) as f:
        soup = BeautifulSoup(f, 'html.parser')
        # æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰ "â™‚" çš„ <a> æ ‡ç­¾
        for a_tag in soup.find_all('a'):
            # æ£€æŸ¥<a>æ ‡ç­¾åæ˜¯å¦æœ‰ â™‚
            if a_tag.next_sibling and "â™‚" in a_tag.next_sibling:
                m_list.append(a_tag.string.strip('\u200e'))
    with open(json_path, 'w') as j:
        json.dump({"content" : m_list}, j, indent=4)
    return m_list

class EHentaiTools:
    def __init__(self, cookie, logger=None):
        self.cookie = cookie
        self.logger = logger
        self.session = requests.Session()
        self.session.headers.update(headers)
        self.session.cookies.update(cookie)
        self.favcat_map = {}
    
    def _normalize_time(self, time_str: str) -> str:
        """
        æ ‡å‡†åŒ–æ—¶é—´å­—ç¬¦ä¸²ä¸ºISO 8601æ ¼å¼ï¼Œä¾¿äºå¯é æ¯”è¾ƒ
        å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸå­—ç¬¦ä¸²å¹¶è®°å½•è­¦å‘Š
        """
        if not time_str or not isinstance(time_str, str):
            return ""
        
        time_str = time_str.strip()
        
        # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
        if re.match(r'^\d{4}-\d{2}-\d{2}( \d{2}:\d{2})?$', time_str):
            return time_str
        
        # å°è¯•è§£æå…¶ä»–å¯èƒ½çš„æ ¼å¼
        try:
            # å°è¯•å¸¸è§æ ¼å¼
            for fmt in ["%Y-%m-%d %H:%M", "%Y-%m-%d", "%d %b %Y", "%b %d, %Y"]:
                try:
                    dt = datetime.strptime(time_str, fmt)
                    return dt.strftime("%Y-%m-%d %H:%M")
                except ValueError:
                    continue
        except Exception as e:
            if self.logger:
                self.logger.warning(f"æ— æ³•æ ‡å‡†åŒ–æ—¶é—´å­—ç¬¦ä¸² '{time_str}': {e}")
        
        # æ— æ³•è§£æï¼Œè¿”å›åŸå­—ç¬¦ä¸²
        return time_str

    def _check_url(self, url, name, error_msg, success_msg, keyword=None):
        try:
            response = self.session.get(url, allow_redirects=True, timeout=10)
            final_url = response.url.lower()
            valid = True
            eh_funds = None
            if 'login' in final_url or (keyword and keyword not in final_url):
                valid = False
                if self.logger:
                    self.logger.error(error_msg)
            else:
                if name == "E-Hentai":
                    eh_funds = self.get_funds(response.text)
                if self.logger:
                    self.logger.info(success_msg)
            return valid, eh_funds
        except Exception as e:
            if self.logger:
                self.logger.warning(f"æ— æ³•æ‰“å¼€ {url}, è¯·æ£€æŸ¥ç½‘ç»œ: {e}")
            return None, None

    def is_valid_cookie(self):
        # å…ˆæ£€æŸ¥ E-Hentai
        eh_valid, eh_funds = self._check_url(
            "https://e-hentai.org/home.php",
            "E-Hentai",
            "æ— æ³•è®¿é—® https://e-hentai.org/home.php, Archive ä¸‹è½½åŠŸèƒ½å°†ä¸å¯ç”¨",
            "æˆåŠŸè®¿é—® https://e-hentai.org/home.php, Archive ä¸‹è½½åŠŸèƒ½å¯ç”¨"
        )
        # å†æ£€æŸ¥ ExHentai
        exh_valid, _ = self._check_url(
            "https://exhentai.org/uconfig.php",
            "ExHentai",
            "æ— æ³•è®¿é—® https://exhentai.org/uconfig.php, ExHentai ä¸‹è½½å¯èƒ½å—é™",
            "æˆåŠŸè®¿é—® https://exhentai.org/uconfig.php, ExHentai ä¸‹è½½åŠŸèƒ½å¯ç”¨",
            keyword="uconfig"
        )

        return eh_valid, exh_valid, eh_funds

    def get_funds(self, html_text):
        soup = BeautifulSoup(html_text, 'html.parser')
        h2_tag = soup.find('h2', string='Total GP Gained')
        if h2_tag:
            homebox_div = h2_tag.find_next_sibling('div', class_='homebox')
            if homebox_div:
                table = homebox_div.find('table')
                if table:
                    total_gp = 0
                    # éå†è¡¨æ ¼ä¸­çš„æ¯ä¸€è¡Œ
                    for row in table.find_all('tr'):
                        # GP æ•°å€¼é€šå¸¸åœ¨æ¯è¡Œçš„ç¬¬ä¸€ä¸ª <td> æ ‡ç­¾ä¸­
                        td = row.find('td')
                        if td:
                            gp_text = td.get_text(strip=True)
                            if gp_text:
                                try:
                                    # ç§»é™¤é€—å·å¹¶è½¬æ¢ä¸ºæ•´æ•°
                                    total_gp += int(gp_text.replace(',', ''))
                                except ValueError:
                                    # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå¯èƒ½ä¸æ˜¯æ•°å­—ï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡
                                    if self.logger:
                                        self.logger.warning(f"æ— æ³•ä» '{gp_text}' ä¸­è§£æ GP æ•°å€¼")
                    return total_gp
        return None

    # ä» E-Hentai API è·å–ç”»å»Šä¿¡æ¯
    def get_gmetadata(self, url):
        API = 'https://api.e-hentai.org/api.php'
        searchUrl = re.search(r'g\/(\d+?)\/(.+?)(\/|$)', url)
        if not searchUrl == None:
            gid = searchUrl.group(1)
            gtoken = searchUrl.group(2)
            data = {
                "method": "gdata",
                "gidlist": [
                    [gid,gtoken]
                ],
                "namespace": 1
                }
            response = self.session.post(API,json=data)
            if response.status_code == 200:
                if self.logger: self.logger.info(response.json())
                gmetadata_dir = check_dirs(os.path.join('data', 'ehentai', 'gmetadata'))
                with open(os.path.join(gmetadata_dir, f'{gid}.json'), 'w+', encoding='utf-8') as wf:
                    json.dump(response.json(),wf,ensure_ascii=False,indent=4)
                return response.json()['gmetadata'][0]
        else:
            if self.logger: self.logger.error(f'è§£æ{url}æ—¶é‡åˆ°äº†é”™è¯¯')

    def _download(self, url, path, task_id=None, tasks=None, tasks_lock=None):
        try:
            with self.session.get(url, stream=True, timeout=30) as r:
                r.raise_for_status()
                total_size = int(r.headers.get('content-length', 0))
                downloaded = 0

                with open(path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
                        if task_id and tasks and tasks_lock:
                            with tasks_lock:
                                task = tasks.get(task_id)
                                if task and task.cancelled:
                                    if self.logger:
                                        self.logger.info(f"ä»»åŠ¡ {task_id} è¢«ç”¨æˆ·å–æ¶ˆï¼Œæ­£åœ¨æ¸…ç†æ–‡ä»¶")
                                    # åˆ é™¤å·²ä¸‹è½½çš„æ–‡ä»¶
                                    if os.path.exists(path):
                                        os.remove(path)
                                    # è¿”å›Noneè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…ä¸­æ–­çº¿ç¨‹
                                    return None

                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)

                            # æ›´æ–°è¿›åº¦ä¿¡æ¯
                            if task_id and tasks and tasks_lock:
                                progress = 0
                                if total_size > 0:
                                    progress = min(100, int((downloaded / total_size) * 100))

                                with tasks_lock:
                                    if task_id in tasks:
                                        tasks[task_id].progress = progress
                                        tasks[task_id].downloaded = downloaded
                                        tasks[task_id].total_size = total_size
                                        # ç›´æ¥ä¸‹è½½æ¨¡å¼æ— æ³•è·å–å®æ—¶é€Ÿåº¦ï¼Œè®¾ç½®ä¸º0
                                        tasks[task_id].speed = 0
                if self.logger:
                    self.logger.info(f"ä¸‹è½½å®Œæˆ: {path}")
                print(f"ä¸‹è½½å®Œæˆ: {path}")
                return path
        except Exception as e:
            if self.logger:
                self.logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            return None
        
    def _download_torrent(self, torrent_url, torrent_name):
        try:
            if self.logger: self.logger.info(f"å°è¯•ä¸‹è½½ç§å­: {torrent_url}")
            response = self.session.get(torrent_url, timeout=10)
            
            # å³ä½¿ gid é”™è¯¯, ehtracker ä¹Ÿä¼šè¿”å› 200, ä½†å†…å®¹æ˜¯ HTMLã€‚
            # å› æ­¤éœ€è¦é€šè¿‡ Content-Type æ¥åˆ¤æ–­æ˜¯å¦æ˜¯çœŸæ­£çš„ç§å­æ–‡ä»¶ã€‚
            content_type = response.headers.get('Content-Type', '').lower()
            if response.status_code == 200 and 'text/html' not in content_type:
                torrents_dir = check_dirs(os.path.join('.', 'data', 'ehentai', 'torrents'))
                torrent_path = os.path.join(torrents_dir, torrent_name)
                with open(torrent_path, 'wb') as f:
                    f.write(response.content)
                
                # åœ¨æ–‡ä»¶ç³»ç»Ÿå±‚é¢éªŒè¯æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
                if os.path.isfile(torrent_path) and os.path.getsize(torrent_path) > 0:
                    if self.logger: self.logger.info(f"ç§å­ä¸‹è½½å¹¶éªŒè¯æˆåŠŸ: {torrent_path}")
                    return torrent_path
                else:
                    if self.logger: self.logger.warning(f"ç§å­æ–‡ä»¶å†™å…¥å¤±è´¥æˆ–ä¸ºç©º: {torrent_path}")
                    return None
            else:
                if self.logger:
                    self.logger.warning(f"ä¸‹è½½ç§å­å¤±è´¥(gidå¯èƒ½æ— æ•ˆ): {torrent_url}, status: {response.status_code}, content-type: {content_type}")
                return None
        except requests.RequestException as e:
            if self.logger: self.logger.error(f"ä¸‹è½½ç§å­æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {torrent_url}, error: {e}")
            return None

    def get_deleted_gallery_torrent(self, gmetadata):
        if not (gmetadata and gmetadata.get('torrents')):
            return
        torrents_list = gmetadata['torrents']
        if not torrents_list:
            if self.logger: self.logger.info("è¯¥ç”»å»Šæ²¡æœ‰å¯ç”¨çš„ç§å­æ–‡ä»¶ã€‚")
            return

        # ä»æœ€æ–°åˆ°æœ€æ—§æ’åºç§å­, ä¼˜å…ˆå°è¯•æœ€æ–°çš„
        sorted_torrents = sorted(torrents_list, key=lambda t: int(t.get('added', '0')), reverse=True)
        
        # å‡†å¤‡è¦å°è¯•çš„ gids, ä¿æŒé¡ºåº
        ordered_gids = [
            gmetadata.get('gid'),
            gmetadata.get('parent_gid'),
            gmetadata.get('first_gid')
        ]
        valid_gids = [gid for gid in ordered_gids if gid is not None]

        torrent_path = None
        # å¤–å±‚å¾ªç¯: éå†æ‰€æœ‰ç§å­ (ä»æ–°åˆ°æ—§)
        for torrent in sorted_torrents:
            torrent_hash = torrent.get('hash')
            torrent_name = torrent.get('name')

            if not (torrent_hash and torrent_name):
                continue # è·³è¿‡æ— æ•ˆçš„ç§å­æ¡ç›®

            # å†…å±‚å¾ªç¯: éå†æ‰€æœ‰æœ‰æ•ˆçš„ gid
            for gid in valid_gids:
                torrent_url = f'https://ehtracker.org/get/{gid}/{torrent_hash}.torrent'
                torrent_path = self._download_torrent(torrent_url=torrent_url, torrent_name=torrent_name)
                if torrent_path:
                    if self.logger:
                        self.logger.info(f"æˆåŠŸä¸‹è½½ç§å­: ä½¿ç”¨ gid={gid} å’Œ hash={torrent_hash}")
                    break  # æˆåŠŸ, é€€å‡ºå†…å±‚å¾ªç¯
                
                # ä¸ºé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›, åœ¨æ¯æ¬¡å¤±è´¥çš„å°è¯•åå¢åŠ 1ç§’å»¶è¿Ÿ
                time.sleep(1)
            
            if torrent_path:
                return torrent_path
        
        if not torrent_path and self.logger:
            self.logger.warning(f"å°è¯•äº† {len(sorted_torrents)} ä¸ªç§å­å’Œ {len(valid_gids)} ä¸ªgids, ä»æœªæ‰¾åˆ°å¯ç”¨çš„ç§å­æ–‡ä»¶ã€‚")

    def get_download_link(self, url, mode):
        response = self.session.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹è­¦å‘Š
            h1 = soup.find('h1')
            if h1 and h1.text == 'Content Warning':
                if self.logger: self.logger.info("æ£€æµ‹åˆ°å†…å®¹è­¦å‘Š, é€‰æ‹©å¿½ç•¥å¹¶å°è¯•é‡æ–°åŠ è½½")
                response = self.session.get(url + '/?nw=always')
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                else:
                    if self.logger: self.logger.error("æ·»åŠ  'nw=always' å‚æ•°åè¯·æ±‚å¤±è´¥ï¼Œè¯·ä»”ç»†è”ç³»è„šæœ¬ç»´æŠ¤è€…æ’æŸ¥é—®é¢˜")
                    return None, None
            # å…ˆçœ‹çœ‹ Torrent æƒ…å†µ
            if not mode == 'archive':
                torrent_a_tag = soup.find("a", string=lambda text: text and "Torrent Download" in text,onclick=True)
                # æå–é“¾æ¥
                if torrent_a_tag:
                    # å¯ç›´æ¥é€šè¿‡ torrent_a_tag.text ä¸­çš„æ•°å­—åˆ¤æ–­æ˜¯å¦æœ‰ç§å­
                    # è·å–onclickå±æ€§ä¸­çš„URL
                    onclick_value = torrent_a_tag['onclick']
                    search_number = re.search(r'(\d+)', torrent_a_tag.text)
                    if not search_number == None: torrent_count = int(search_number.group(1))
                    if self.logger: self.logger.info(f"æ‰¾åˆ° {torrent_count} ä¸ªç§å­")
                    if torrent_count > 0:
                        # æå–URL
                        start_idx = onclick_value.find("'") + 1
                        end_idx = onclick_value.find("'", start_idx)
                        torrent_window_url = onclick_value[start_idx:end_idx]
                        torrent_list = {}
                        # è¯·æ±‚ torrent_list_url
                        response = self.session.get(torrent_window_url)
                        if response.status_code == 200:
                            text = response.text
                            search_outdated_text = re.search(r'(.+)<p.+Outdated Torrent', text, re.S)
                            if not search_outdated_text == None:
                                if self.logger: self.logger.info('å‘ç° Outdated Torrent')
                                t_html = search_outdated_text.group(1)
                            else: t_html = text
                            t_soup = BeautifulSoup(t_html, 'html.parser')
                            form_list = t_soup.find_all('form', method="post")
                            torrent_list = []
                            for form in form_list:
                                for td in form.find_all('td'):
                                    a_tags_with_onclick = form.find('a', onclick=True)
                                    if a_tags_with_onclick:
                                        torrent_name = a_tags_with_onclick.text + '.torrent'
                                        torrent_link = a_tags_with_onclick['href']
                                    for span in td.find_all('span'):
                                        if 'Seeds' in td.text:
                                            seeds_count = re.search(r'(\d+)', td.text).group(1)
                                torrent_list.append({'name':torrent_name, 'link':torrent_link, 'count':int(seeds_count)})
                            # è¾¹ç¼˜æƒ…å†µå¤„ç†: æ£€æŸ¥ torrent_list æ˜¯å¦ä¸ºç©ºä»¥é˜²æ­¢ ValueError
                            if torrent_list:
                                # å¯è¯»æ€§æ”¹è¿›: lambda ä¸­ä½¿ç”¨æ›´å…·æè¿°æ€§çš„å˜é‡å `torrent`
                                # æœ€ä½³å®è·µ: .get() æ–¹æ³•å¯ä»¥å®‰å…¨åœ°å¤„ç†ç¼ºå°‘ 'count' é”®çš„æƒ…å†µ
                                max_seeds_torrent = max(torrent_list, key=lambda torrent: torrent.get('count', 0))
                                if self.logger: self.logger.info(f"å…±æ‰¾åˆ°{len(torrent_list)}ä¸ªæœ‰æ•ˆç§å­, æœ¬æ¬¡é€‰æ‹©, {max_seeds_torrent}")
                                
                                # å°†ç§å­ä¸‹è½½è‡³æœ¬åœ°
                                torrent_path = self._download_torrent(torrent_url=max_seeds_torrent['link'], torrent_name=max_seeds_torrent['name'])
                                # å†å°†ç§å­æ¨é€åˆ° aria2, ç§å­å°†ä¼šä¸‹è½½åˆ° dir
                                return 'torrent', torrent_path
                            else:
                                if self.logger: self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç§å­ã€‚")
                # è·å– Archive
                if self.logger: self.logger.info('\nå¼€å§‹è¿›è¡Œ Archive Download')
                archive_a_tag = soup.find("a", string="Archive Download",onclick=True)
                # æå–é“¾æ¥
                if archive_a_tag:
                    # è·å–onclickå±æ€§ä¸­çš„URL
                    onclick_value = archive_a_tag['onclick']
                    # æå–URL
                    start_idx = onclick_value.find("'") + 1
                    end_idx = onclick_value.find("'", start_idx)
                    download_url = onclick_value[start_idx:end_idx]
                    if self.logger: self.logger.info(f"Archive Download Link: {download_url}")
                else:
                    if self.logger: self.logger.info("No matching element found.")
                data = {
                    "dltype":"org",
                    "dlcheck":"Download Original Archive"
                }
                try:
                    response = self.session.post(download_url, data)
                    a_soup = BeautifulSoup(response.text, 'html.parser')
                    # æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰ onclick å±æ€§çš„ <a> æ ‡ç­¾
                    a_tags_with_onclick = a_soup.find_all('a', onclick=True)
                    # æå– href å±æ€§å†…å®¹
                    hrefs = [a['href'] for a in a_tags_with_onclick]
                    base_url = hrefs[0].replace("?autostart=1", "")
                    final_url = base_url + '?start=1'
                    if self.logger: self.logger.info(f"å¼€å§‹ä¸‹è½½: {final_url}")
                    # è¿”å›ç§å­åœ°å€
                    return 'archive', final_url
                except Exception as e:
                    # å¦‚æœå‘ç”Ÿäº†ç‰¹å®šç±»å‹çš„å¼‚å¸¸ï¼Œæ‰§è¡Œè¿™é‡Œçš„ä»£ç 
                    if self.logger: self.logger.info(f"An error occurred: {e}")

    LAYOUT_SELECTORS = {
        "thumbnail": 'div[class^="itg gld"]',
        "minimal": 'table[class^="itg gltm"]',
        "compact": 'table[class^="itg gltc"]',
        "extended": 'table[class^="itg glte"]',
    }

    def _get_layout(self, soup: BeautifulSoup) -> str:
        for layout, selector in self.LAYOUT_SELECTORS.items():
            if soup.select_one(selector):
                return layout

    def _build_favcat_map(self, soup: BeautifulSoup) -> dict:
        favcat_map = {}
        fav_elements = soup.select('div.nosel div.fp[onclick]')
        for element in fav_elements:
            name_div = element.select_one('div:nth-of-type(3)')
            if not name_div:
                continue
            category_name = name_div.get_text(strip=True)
            onclick_attr = element.get('onclick', '')
            match = re.search(r"favcat=(\d+)", onclick_attr)
            if match:
                favcat_id = match.group(1)
                favcat_map[favcat_id] = category_name
        
        if favcat_map:
            # ä½¿ç”¨ update ä¿ç•™æ—§æ•°æ®ï¼Œä»¥é˜²é¡µé¢ä¸å®Œæ•´
            self.favcat_map.update(favcat_map)

        return favcat_map

    def _extract_thumbnail_galleries(self, soup: BeautifulSoup) -> list:
        galleries = []
        gallery_elements = soup.select('div.gl1t')
        for gallery in gallery_elements:
            info = {}
            title_element = gallery.select_one('a > span.glink')
            if title_element:
                info['title'] = title_element.get_text(strip=True)
                info['url'] = title_element.parent['href']
            thumb_element = gallery.select_one('div.gl3t img')
            if thumb_element:
                info['thumbnail_url'] = thumb_element['src']
            gl5t_divs = gallery.select('div.gl5t > div > div')
            for div in gl5t_divs:
                if 'cs' in div.get('class', []):
                    info['category'] = div.get_text(strip=True)
                elif div.get('id', '').startswith('posted_'):
                    time_text = div.get_text(strip=True)
                    info['posted_date'] = time_text
                    info['favcat_title'] = div.get('title', '')
                    info['added'] = self._normalize_time(time_text)
                elif 'pages' in div.get_text(strip=True):
                    info['pages'] = div.get_text(strip=True)
            tags = gallery.select('div.gl6t > div.gt')
            info['tags'] = [tag.get('title', '') for tag in tags]
            if info:
                galleries.append(info)
        return galleries

    def _extract_minimal_galleries(self, soup: BeautifulSoup) -> list:
        galleries = []
        gallery_elements = soup.select('table[class^="itg gltm"] tr')
        for row in gallery_elements:
            if not row.find('td', class_='gl1m'):
                continue
            info = {}
            title_element = row.select_one('td.gl3m a > div.glink')
            if title_element:
                info['title'] = title_element.get_text(strip=True)
                info['url'] = title_element.parent['href']
            thumb_element = row.select_one('div.glthumb img')
            if thumb_element:
                info['thumbnail_url'] = thumb_element.get('data-src') or thumb_element.get('src')
            category_element = row.select_one('td.gl1m.glcat > div.cs')
            if category_element:
                info['category'] = category_element.get_text(strip=True)
            posted_element = row.select_one('td.gl2m > div[id^="posted_"]')
            if posted_element:
                time_text = posted_element.get_text(strip=True)
                info['posted_date'] = time_text
                info['favcat_title'] = posted_element.get('title', '')
                info['added'] = self._normalize_time(time_text)
            tags = row.select('div.gltm > div.gt')
            info['tags'] = [tag.get('title', '') for tag in tags]
            if info:
                galleries.append(info)
        return galleries

    def _extract_compact_galleries(self, soup: BeautifulSoup) -> list:
        galleries = []
        gallery_elements = soup.select('table[class^="itg gltc"] tr')
        for row in gallery_elements:
            if not row.find('td', class_='gl1c'):
                continue
            info = {}
            title_element = row.select_one('td.gl3c a > div.glink')
            if title_element:
                info['title'] = title_element.get_text(strip=True)
                info['url'] = title_element.parent['href']
            thumb_element = row.select_one('div.glthumb img')
            if thumb_element:
                info['thumbnail_url'] = thumb_element.get('data-src') or thumb_element.get('src')
            category_element = row.select_one('td.gl1c.glcat > div.cn')
            if category_element:
                info['category'] = category_element.get_text(strip=True)
            posted_element = row.select_one('td.gl2c > div > div[id^="posted_"]')
            if posted_element:
                time_text = posted_element.get_text(strip=True)
                info['posted_date'] = time_text
                info['favcat_title'] = posted_element.get('title', '')
                info['added'] = self._normalize_time(time_text)
            tags = row.select('td.gl3c.glname div.gt')
            info['tags'] = [tag.get('title', '') for tag in tags]
            authors = [tag.text for tag in tags if tag.get('title', '').startswith('artist:')]
            if authors:
                info['author'] = ' / '.join(authors)
            if info:
                galleries.append(info)
        return galleries

    def _extract_extended_galleries(self, soup: BeautifulSoup) -> list:
        galleries = []
        gallery_elements = soup.select('table[class^="itg glte"] tr')
        for row in gallery_elements:
            if not row.find('td', class_='gl1e'):
                continue
            info = {}
            link_element = row.select_one('td.gl1e a')
            if link_element:
                info['url'] = link_element['href']
                thumb_element = link_element.select_one('img')
                if thumb_element:
                    info['title'] = thumb_element.get('title', '')
                    info['thumbnail_url'] = thumb_element['src']
            category_element = row.select_one('div.gl3e div.cn')
            if category_element:
                info['category'] = category_element.get_text(strip=True)
            posted_element = row.select_one('div[id^="posted_"]')
            if posted_element:
                time_text = posted_element.get_text(strip=True)
                info['posted_date'] = time_text
                info['favcat_title'] = posted_element.get('title', '')
                info['added'] = self._normalize_time(time_text)
            tags = row.select('div.gl4e table div[title]')
            info['tags'] = [tag.get('title', '') for tag in tags]
            authors = [tag.text for tag in tags if tag.get('title', '').startswith('artist:')]
            if authors:
                info['author'] = ' / '.join(authors)
            if info:
                galleries.append(info)
        return galleries

    def _parse_favorites_page(self, soup: BeautifulSoup) -> tuple[str, list]:
        layout = self._get_layout(soup)
        self._build_favcat_map(soup)  # æ›´æ–°æ”¶è—å¤¹åˆ—è¡¨ç¼“å­˜
        galleries_data = []
        if layout == 'thumbnail':
            galleries_data = self._extract_thumbnail_galleries(soup)
        elif layout == 'minimal':
            galleries_data = self._extract_minimal_galleries(soup)
        elif layout == 'compact':
            galleries_data = self._extract_compact_galleries(soup)
        elif layout == 'extended':
            galleries_data = self._extract_extended_galleries(soup)

        # ä»é¡µé¢ä¸­æå–æ¯ä¸ªç”»å»Šçš„ favcat
        # åˆ›å»ºæ”¶è—å¤¹åç§°åˆ°IDçš„åå‘æ˜ å°„
        name_to_id = {name: fav_id for fav_id, name in self.favcat_map.items()}
        
        if galleries_data:
            for gallery in galleries_data:
                # favcat_title å­˜å‚¨çš„æ˜¯æ”¶è—å¤¹åç§°ï¼Œå¦‚ "Common", "ğŸ’•" ç­‰
                # é€šè¿‡åç§°åæŸ¥ favcat ID
                favcat_name = gallery.get('favcat_title', '')
                if favcat_name and favcat_name in name_to_id:
                    gallery['favcat'] = name_to_id[favcat_name]
                else:
                    gallery['favcat'] = None
        return layout, galleries_data

    def get_favcat_list(self) -> list:
        """è·å–ç”¨æˆ·æ”¶è—å¤¹åˆ—è¡¨, å¦‚æœç¼“å­˜ä¸ºç©ºåˆ™ä¸»åŠ¨è·å–"""
        if not self.favcat_map:
            if self.logger:
                self.logger.info("æ”¶è—å¤¹ç¼“å­˜ä¸ºç©º, æ­£åœ¨ä¸»åŠ¨è·å–...")
            url = "https://exhentai.org/favorites.php"
            try:
                response = self.session.get(url, allow_redirects=True, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    # _build_favcat_map ä¼šè‡ªåŠ¨æ›´æ–° self.favcat_map
                    self._build_favcat_map(soup)
                    if self.logger:
                        self.logger.info(f"æˆåŠŸè·å–å¹¶ç¼“å­˜äº† {len(self.favcat_map)} ä¸ªæ”¶è—å¤¹åˆ†ç±»ã€‚")
                else:
                    if self.logger:
                        self.logger.error(f"è·å–æ”¶è—å¤¹åˆ—è¡¨å¤±è´¥: status_code={response.status_code}")
            except requests.RequestException as e:
                if self.logger:
                    self.logger.error(f"è·å–æ”¶è—å¤¹åˆ—è¡¨æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")

        # æ— è®ºå¦‚ä½•éƒ½ä»å½“å‰ç¼“å­˜è¿”å›
        # è½¬æ¢æˆå‰ç«¯éœ€è¦çš„æ ¼å¼ [{'id': k, 'name': 'k: v'}, ...]
        favcat_list = [
            {'id': k, 'name': f"{k}: {v}"}
            for k, v in self.favcat_map.items()
        ]
        
        if favcat_list:
            favcat_list.sort(key=lambda x: int(x['id']))
        return favcat_list

    def get_favorites(self, favcat_list: list, existing_gids: set = None, initial_scan_pages: int = 1) -> list:
        """
        è·å–æ”¶è—å¤¹ç”»å»Šåˆ—è¡¨
        
        Args:
            favcat_list: è¦åŒæ­¥çš„æ”¶è—å¤¹åˆ†ç±»IDåˆ—è¡¨
            existing_gids: æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„GIDé›†åˆï¼Œç”¨äºå¢é‡æ‰«æ
            initial_scan_pages: é¦–æ¬¡æ‰«æé¡µæ•°ï¼Œ0è¡¨ç¤ºå…¨é‡æ‰«æï¼Œå…¶ä»–æ•°å­—è¡¨ç¤ºæ‰«ææŒ‡å®šé¡µæ•°
            
        Returns:
            ç”»å»Šåˆ—è¡¨
        """
        all_galleries = []
        stop_scanning = False
        consecutive_matches = 0  # è¿ç»­åŒ¹é…è®¡æ•°å™¨
        MATCH_THRESHOLD = 5  # å›ºå®šè¿ç»­åŒ¹é…é˜ˆå€¼
        
        # åˆ¤æ–­æ‰«ææ¨¡å¼
        if existing_gids is None or len(existing_gids) == 0:
            if initial_scan_pages == 0:
                scan_mode = "full_scan"
                max_pages = None  # æ— é™åˆ¶
                if self.logger:
                    self.logger.info("æ•°æ®åº“ä¸ºç©ºï¼Œå°†è¿›è¡Œå…¨é‡æ‰«æï¼ˆæ‰€æœ‰é¡µï¼‰")
            else:
                scan_mode = "initial_scan"
                max_pages = initial_scan_pages
                if self.logger:
                    self.logger.info(f"æ•°æ®åº“ä¸ºç©ºï¼Œå°†æ‰«æå‰ {initial_scan_pages} é¡µ")
        elif len(existing_gids) < MATCH_THRESHOLD:
            if initial_scan_pages == 0:
                scan_mode = "full_scan"
                max_pages = None
                if self.logger:
                    self.logger.info(f"æ•°æ®åº“ä¸­åªæœ‰ {len(existing_gids)} ä¸ªè®°å½•ï¼ˆå°‘äº{MATCH_THRESHOLD}ä¸ªï¼‰ï¼Œå°†è¿›è¡Œå…¨é‡æ‰«æ")
            else:
                scan_mode = "initial_scan"
                max_pages = initial_scan_pages
                if self.logger:
                    self.logger.info(f"æ•°æ®åº“ä¸­åªæœ‰ {len(existing_gids)} ä¸ªè®°å½•ï¼ˆå°‘äº{MATCH_THRESHOLD}ä¸ªï¼‰ï¼Œå°†æ‰«æå‰ {initial_scan_pages} é¡µ")
        else:
            scan_mode = "incremental"
            max_pages = None  # å¢é‡æ‰«æä¸é™åˆ¶é¡µæ•°ï¼Œç”±åŒ¹é…é˜ˆå€¼æ§åˆ¶
            if self.logger:
                self.logger.info(f"æ•°æ®åº“ä¸­æœ‰ {len(existing_gids)} ä¸ªè®°å½•ï¼Œå°†è¿›è¡Œå¢é‡æ‰«æï¼ˆè¿ç»­åŒ¹é…{MATCH_THRESHOLD}ä¸ªæ—¶åœæ­¢ï¼‰")
        
        # ä»æ”¶è—å¤¹é¦–é¡µå¼€å§‹, å¼ºåˆ¶æŒ‰æ”¶è—æ—¶é—´æ’åº
        url = "https://exhentai.org/favorites.php?inline_set=fs_f"
        page_count = 0

        while url and not stop_scanning:
            page_count += 1
            if self.logger:
                self.logger.info(f"æ­£åœ¨è·å–æ”¶è—å¤¹é¡µé¢ {page_count}: {url}")
            
            try:
                response = self.session.get(url, allow_redirects=True, timeout=10)
                if response.status_code != 200:
                    if self.logger:
                        self.logger.error(f"è·å–æ”¶è—å¤¹é¡µé¢å¤±è´¥: {url}, status_code: {response.status_code}")
                    break
                
                soup = BeautifulSoup(response.text, 'html.parser')
                # ä»é¡µé¢ä¸­æå–ç”»å»Šä¿¡æ¯
                _, galleries_data = self._parse_favorites_page(soup)
                
                if galleries_data:
                    for gallery in galleries_data:
                        # æ£€æŸ¥ç”»å»Šæ˜¯å¦å±äºæŒ‡å®šçš„ favcat
                        if gallery.get('favcat') not in favcat_list:
                            continue
                        
                        # æå– GID
                        from utils import parse_gallery_url
                        gid, _ = parse_gallery_url(gallery.get('url', ''))
                        
                        if scan_mode == "incremental" and existing_gids and gid:
                            # å¢é‡æ‰«ææ¨¡å¼
                            if gid in existing_gids:
                                consecutive_matches += 1
                                if self.logger:
                                    self.logger.info(f"å‘ç°å·²å­˜åœ¨çš„ GID {gid}ï¼Œè¿ç»­åŒ¹é…è®¡æ•°: {consecutive_matches}/{MATCH_THRESHOLD}")
                                
                                # è¾¾åˆ°é˜ˆå€¼ï¼Œåœæ­¢æ‰«æ
                                if consecutive_matches >= MATCH_THRESHOLD:
                                    stop_scanning = True
                                    if self.logger:
                                        self.logger.info(f"è¿ç»­åŒ¹é… {MATCH_THRESHOLD} ä¸ªå·²å­˜åœ¨çš„ GIDï¼Œåœæ­¢å¢é‡æ‰«æã€‚")
                                    break
                            else:
                                # é‡åˆ°æ–°ç”»å»Šï¼Œé‡ç½®è®¡æ•°å™¨å¹¶æ·»åŠ 
                                consecutive_matches = 0
                                all_galleries.append(gallery)
                        else:
                            # first_page æ¨¡å¼ï¼Œç›´æ¥æ·»åŠ 
                            all_galleries.append(gallery)
                
                if stop_scanning:
                    break
                
                # initial_scan æ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦è¾¾åˆ°é¡µæ•°é™åˆ¶
                if scan_mode == "initial_scan" and max_pages is not None:
                    if page_count >= max_pages:
                        if self.logger:
                            self.logger.info(f"å·²æ‰«æ {page_count} é¡µï¼Œè¾¾åˆ°é…ç½®çš„é¡µæ•°é™åˆ¶ï¼Œåœæ­¢æ‰«æã€‚")
                        break

                # æŸ¥æ‰¾ä¸‹ä¸€é¡µé“¾æ¥
                next_link = soup.select_one('div.searchnav a#dnext')
                if next_link and next_link.get('href'):
                    url = next_link['href']
                    time.sleep(10) # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                else:
                    url = None # æ²¡æœ‰ä¸‹ä¸€é¡µäº†

            except requests.RequestException as e:
                if self.logger:
                    self.logger.error(f"è·å–æ”¶è—å¤¹é¡µé¢æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {url}, error: {e}")
                break
        
        return all_galleries

    def add_to_favorites(self, gid: int, token: str, favcat: str = '1', note: str = '') -> bool:
        """å°†ç”»å»Šæ·»åŠ åˆ°æ”¶è—å¤¹"""
        url = f"https://e-hentai.org/gallerypopups.php?gid={gid}&t={token}&act=addfav"
        form_data = {
            "favcat": favcat,
            "apply": "Apply Changes",
            "favnote": note,
            "update": "1"
        }
        try:
            response = self.session.post(url, data=form_data, timeout=10)
            if response.status_code == 200:
                if self.logger:
                    self.logger.info(f"æˆåŠŸå°† gid={gid} æ·»åŠ åˆ°æ”¶è—å¤¹ (favcat={favcat})")
                return True
            else:
                if self.logger:
                    self.logger.error(f"å°† gid={gid} æ·»åŠ åˆ°æ”¶è—å¤¹å¤±è´¥: status_code={response.status_code}, response={response.text}")
                return False
        except requests.RequestException as e:
            if self.logger:
                self.logger.error(f"å°† gid={gid} æ·»åŠ åˆ°æ”¶è—å¤¹æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
            return False

    def delete_from_favorites(self, gid: str) -> bool:
        """ä»æ”¶è—å¤¹ä¸­åˆ é™¤ç”»å»Š"""
        url = "https://e-hentai.org/favorites.php"
        form_data = {
            "ddact": "delete",
            "modifygids[]": str(gid)
        }
        try:
            response = self.session.post(url, data=form_data, timeout=10)
            if response.status_code == 200:
                if self.logger:
                    self.logger.info(f"æˆåŠŸä»æ”¶è—å¤¹ä¸­åˆ é™¤ gid={gid}")
                return True
            else:
                if self.logger:
                    self.logger.error(f"ä»æ”¶è—å¤¹ä¸­åˆ é™¤ gid={gid} å¤±è´¥: status_code={response.status_code}, response={response.text}")
                return False
        except requests.RequestException as e:
            if self.logger:
                self.logger.error(f"ä»æ”¶è—å¤¹ä¸­åˆ é™¤ gid={gid} æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
            return False